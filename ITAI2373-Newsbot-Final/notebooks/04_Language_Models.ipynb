{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd068b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üåê Language Detection and Processing\n",
    "# TODO: Implement multilingual capabilities\n",
    "\n",
    "from langdetect import detect_langs, LangDetectException\n",
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import SentenceTransformer # Added this import\n",
    "import numpy as np\n",
    "import networkx as nx # Assuming networkx is used in EntityRelationshipMapper which is called by this class\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "class MultilingualProcessor:\n",
    "    \"\"\"\n",
    "    Advanced multilingual processing with language detection and cultural context.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize models and data\n",
    "\n",
    "        self.embedding_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "        # Idioms and cultural references per language\n",
    "        self.idioms = {\n",
    "            'en': ['break the ice', 'spill the beans', 'kick the bucket'],\n",
    "            'es': ['poner los puntos sobre las √≠es', 'estar en las nubes'],\n",
    "            'fr': ['poser un lapin', 'donner sa langue au chat']\n",
    "        }\n",
    "\n",
    "        self.regional_keywords = {\n",
    "            'en': ['New York', 'Wall Street', 'Thanksgiving'],\n",
    "            'es': ['Madrid', 'La Tomatina', 'Santiago'],\n",
    "            'fr': ['Paris', 'Bastille', 'Lyon']\n",
    "        }\n",
    "\n",
    "        # Historical/social nuances keywords (example)\n",
    "        self.social_nuances = {\n",
    "            'en': ['civil rights', 'Black Lives Matter', 'Me Too'],\n",
    "            'es': ['derechos civiles', 'Black Lives Matter', 'Me Too'],\n",
    "            'fr': ['droits civiques', 'Black Lives Matter', 'Me Too']\n",
    "        }\n",
    "\n",
    "    def detect_language(self, text):\n",
    "        \"\"\"\n",
    "        Detect language with confidence scoring.\n",
    "\n",
    "        Handles:\n",
    "        - Multiple languages (best effort)\n",
    "        - Short texts\n",
    "        - Code-switching (detects top languages by probability)\n",
    "\n",
    "        Returns:\n",
    "            List of (language_code, confidence) tuples sorted by confidence descending.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            detections = detect_langs(text)\n",
    "            # Filter out very low confidence languages\n",
    "            filtered = [(det.lang, det.prob) for det in detections if det.prob > 0.1]\n",
    "            filtered.sort(key=lambda x: x[1], reverse=True)\n",
    "            return filtered\n",
    "        except LangDetectException:\n",
    "            return [(\"unknown\", 0.0)]\n",
    "\n",
    "\n",
    "    def translate_text(self, text, target_language='en'):\n",
    "        \"\"\"\n",
    "        Translate text with quality scoring and cultural adaptation hints.\n",
    "\n",
    "        Returns:\n",
    "            dict with 'translated_text', 'quality_score', 'cultural_adaptation_notes'\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            translator = GoogleTranslator(source='auto', target=target_language)\n",
    "            translated = translator.translate(text)\n",
    "\n",
    "\n",
    "            quality_score = self._assess_translation_quality(text, translated)\n",
    "            cultural_adaptation_notes = self._detect_cultural_elements(text, target_language)\n",
    "\n",
    "            return {\n",
    "                'translated_text': translated,\n",
    "                'quality_score': quality_score,\n",
    "                'cultural_adaptation_notes': cultural_adaptation_notes\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'error': str(e), 'translated_text': '', 'quality_score': 0.0, 'cultural_adaptation_notes': []}\n",
    "\n",
    "\n",
    "    def _assess_translation_quality(self, source_text, translated_text):\n",
    "        \"\"\"\n",
    "        Simple heuristic: length ratio, no truncation, presence of common stop words in target language.\n",
    "        \"\"\"\n",
    "\n",
    "        ratio = len(translated_text) / max(len(source_text), 1)\n",
    "        if not (0.7 <= ratio <= 1.3):\n",
    "            return 0.5  # suspect translation quality\n",
    "\n",
    "        # Could be expanded with language-specific stop word check or LM scoring\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "    def _detect_cultural_elements(self, text, target_language):\n",
    "        \"\"\"\n",
    "        Check for cultural idioms or references that might need adaptation.\n",
    "        \"\"\"\n",
    "\n",
    "        notes = []\n",
    "\n",
    "        for lang, idiom_list in self.idioms.items():\n",
    "            for idiom in idiom_list:\n",
    "                if idiom in text.lower():\n",
    "                    notes.append(f\"Contains idiom '{idiom}' from {lang} language; consider cultural adaptation.\")\n",
    "\n",
    "        return notes\n",
    "\n",
    "\n",
    "    def analyze_cross_lingual(self, articles_by_language):\n",
    "        \"\"\"\n",
    "        Compare coverage and perspectives across languages.\n",
    "\n",
    "        Args:\n",
    "            articles_by_language: dict(lang_code -> list of article texts)\n",
    "\n",
    "        Returns:\n",
    "            dict of (lang1, lang2) -> similarity score (cosine similarity of mean embeddings)\n",
    "        \"\"\"\n",
    "\n",
    "        embeddings_by_lang = {}\n",
    "\n",
    "        for lang, articles in articles_by_language.items():\n",
    "            if not articles:\n",
    "                embeddings_by_lang[lang] = np.zeros(self.embedding_model.get_sentence_embedding_dimension())\n",
    "                continue\n",
    "            embeddings = self.embedding_model.encode(articles, convert_to_tensor=True)\n",
    "            mean_emb = embeddings.mean(axis=0).cpu().numpy()\n",
    "            embeddings_by_lang[lang] = mean_emb\n",
    "\n",
    "\n",
    "        lang_codes = list(embeddings_by_lang.keys())\n",
    "        similarity_matrix = {}\n",
    "\n",
    "        for i, lang1 in enumerate(lang_codes):\n",
    "            for j in range(i + 1, len(lang_codes)):\n",
    "                lang2 = lang_codes[j]\n",
    "                emb1 = embeddings_by_lang[lang1]\n",
    "                emb2 = embeddings_by_lang[lang2]\n",
    "                if np.linalg.norm(emb1) == 0 or np.linalg.norm(emb2) == 0:\n",
    "                    sim = 0.0\n",
    "                else:\n",
    "                    sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "                similarity_matrix[(lang1, lang2)] = sim\n",
    "\n",
    "\n",
    "        return similarity_matrix\n",
    "\n",
    "    def extract_cultural_context(self, text, source_language):\n",
    "        \"\"\"\n",
    "        Identify cultural references and context.\n",
    "\n",
    "        Returns dict with:\n",
    "        - idioms_found: list\n",
    "        - regional_references: list\n",
    "        - historical_context: list\n",
    "        - social_nuances: list\n",
    "        \"\"\"\n",
    "\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        idioms_found = [idiom for idiom in self.idioms.get(source_language, [])\n",
    "                        if idiom in text_lower]\n",
    "\n",
    "        regional_refs = [region for region in self.regional_keywords.get(source_language, [])\n",
    "                         if region.lower() in text_lower]\n",
    "\n",
    "        historical_context = self._extract_historical_context(text_lower)\n",
    "\n",
    "        social_nuances_found = [nuance for nuance in self.social_nuances.get(source_language, [])\n",
    "                                if nuance.lower() in text_lower]\n",
    "\n",
    "        return {\n",
    "            'idioms_found': idioms_found,\n",
    "            'regional_references': regional_refs,\n",
    "            'historical_context': historical_context,\n",
    "            'social_nuances': social_nuances_found\n",
    "        }\n",
    "\n",
    "    def _extract_historical_context(self, text_lower):\n",
    "        \"\"\"\n",
    "        Naive implementation: detect presence of years, famous historical event names.\n",
    "\n",
    "        This can be improved by using named entity recognition or knowledge bases.\n",
    "        \"\"\"\n",
    "\n",
    "        # Regex for years 1800-2099\n",
    "        years = re.findall(r'\\b(18|19|20)\\d{2}\\b', text_lower)\n",
    "        years = list(set(years))\n",
    "\n",
    "        # Placeholder events (extend with real DB or API)\n",
    "        events = ['world war', 'cold war', 'industrial revolution', 'french revolution']\n",
    "        found_events = [event for event in events if event in text_lower]\n",
    "\n",
    "\n",
    "        return {'years_mentioned': years, 'events_mentioned': found_events}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processor = MultilingualProcessor()\n",
    "\n",
    "    # Test detect_language\n",
    "    text1 = \"Bonjour, comment √ßa va? Hello!\"\n",
    "    print(\"Language Detection:\", processor.detect_language(text1))\n",
    "\n",
    "    # Test translate_text\n",
    "    print(\"Translation:\", processor.translate_text(\"Je suis tr√®s content.\", target_language='en'))\n",
    "\n",
    "    # Test cross-lingual analysis\n",
    "    articles = {\n",
    "        'en': [\"The economy is growing steadily.\", \"New policies affect markets.\"],\n",
    "        'es': [\"La econom√≠a est√° creciendo lentamente.\", \"Nuevas pol√≠ticas afectan los mercados.\"],\n",
    "        'fr': [\"L'√©conomie conna√Æt une croissance stable.\", \"De nouvelles politiques impactent les march√©s.\"]\n",
    "    }\n",
    "    print(\"Cross-lingual Similarity:\", processor.analyze_cross_lingual(articles))\n",
    "\n",
    "    # Test cultural context extraction\n",
    "    sample_text = \"He decided to break the ice at the party in New York during the Cold War.\"\n",
    "    print(\"Cultural Context:\", processor.extract_cultural_context(sample_text, 'en'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
