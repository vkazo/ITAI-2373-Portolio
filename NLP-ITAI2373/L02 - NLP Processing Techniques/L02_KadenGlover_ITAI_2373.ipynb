{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QBHyofeV98"
      },
      "source": [
        "# Lab 02: Basic NLP Preprocessing Techniques\n",
        "\n",
        "**Course:** ITAI 2373 - Natural Language Processing  \n",
        "**Module:** 02 - Text Preprocessing  \n",
        "**Duration:** 2-3 hours  \n",
        "**Student Name:** Kaden Glover\n",
        "**Date:** June 7\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By completing this lab, you will:\n",
        "1. Understand the critical role of preprocessing in NLP pipelines\n",
        "2. Master fundamental text preprocessing techniques\n",
        "3. Compare different libraries and their approaches\n",
        "4. Analyze the effects of preprocessing on text data\n",
        "5. Build a complete preprocessing pipeline\n",
        "6. Load and work with different types of text datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-L6J3seV99"
      },
      "source": [
        "## üìñ Introduction to NLP Preprocessing\n",
        "\n",
        "Natural Language Processing (NLP) preprocessing refers to the initial steps taken to clean and transform raw text data into a format that's more suitable for analysis by machine learning algorithms.\n",
        "\n",
        "### Why is preprocessing crucial?\n",
        "\n",
        "1. **Standardization:** Ensures consistent text format across your dataset\n",
        "2. **Noise Reduction:** Removes irrelevant information that could confuse algorithms\n",
        "3. **Complexity Reduction:** Simplifies text to focus on meaningful patterns\n",
        "4. **Performance Enhancement:** Improves the efficiency and accuracy of downstream tasks\n",
        "\n",
        "### Real-world Impact\n",
        "Consider searching for \"running shoes\" vs \"Running Shoes!\" - without preprocessing, these might be treated as completely different queries. Preprocessing ensures they're recognized as equivalent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1AajM1neV99"
      },
      "source": [
        "### ü§î Conceptual Question 1\n",
        "**Before we start coding, think about your daily interactions with text processing systems (search engines, chatbots, translation apps). What challenges do you think these systems face when processing human language? List at least 3 specific challenges and explain why each is problematic.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Challenge 1:** Ambiguity:\n",
        "Human language is full of ambiguity. Words and phrases often have multiple meanings depending on context. Text processing systems struggle to correctly interpret meaning without sufficient context, which can lead to misunderstandings or inaccurate responses.\n",
        "\n",
        "**Challenge 2:** Idioms and Figurative Language:\n",
        "Humans often use idioms, metaphors, and other non-literal expressions. These phrases are difficult for systems to interpret because their meanings cannot be derived from the literal definitions of the words. Without cultural or contextual awareness, machines often misinterpret these expressions.\n",
        "\n",
        "\n",
        "\n",
        "**Challenge 3:** Grammar and Spelling Variations:\n",
        "Users often input text with grammar mistakes, slang, or typos‚Äîespecially in informal settings like social media or text messaging. Text processing systems must be able to understand and correct these variations while still preserving the intended meaning, which requires sophisticated language models and error correction mechanisms.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXDC1YX1eV99"
      },
      "source": [
        "## üõ†Ô∏è Part 1: Environment Setup\n",
        "\n",
        "We'll be working with two major NLP libraries:\n",
        "- **NLTK (Natural Language Toolkit):** Comprehensive NLP library with extensive resources\n",
        "- **spaCy:** Industrial-strength NLP with pre-trained models\n",
        "\n",
        "**‚ö†Ô∏è Note:** Installation might take 2-3 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IjSAdym1eV99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3b6b0f-bfd1-4bef-ec3a-df494c2dccac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Installing NLP libraries...\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "print(\"üîß Installing NLP libraries...\")\n",
        "\n",
        "!pip install -q nltk spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0qYPPfkeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 2\n",
        "**Why do you think we need to install a separate language model (en_core_web_sm) for spaCy? What components might this model contain that help with text processing? Think about what information a computer needs to understand English text.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "I think we need to install a separate language model like en_core_web_sm because spaCy by itself doesn‚Äôt know much about the English language.  Basically, the model gives spaCy the \"brain\" it needs to process and make sense of English text.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uqponRiceV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50399162-ef4d-4f29-dfbc-d8aa32508497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Downloading NLTK data packages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ All imports and downloads completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Import Libraries and Download NLTK Data\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Download essential NLTK data\n",
        "print(\"üì¶ Downloading NLTK data packages...\")\n",
        "nltk.download('punkt')      # For tokenization\n",
        "nltk.download('stopwords')  # For stop word removal\n",
        "nltk.download('wordnet')    # For lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
        "\n",
        "print(\"\\n‚úÖ All imports and downloads completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQye-Ye2eV9-"
      },
      "source": [
        "## üìÇ Part 2: Sample Text Data\n",
        "\n",
        "We'll work with different types of text to understand how preprocessing affects various text styles:\n",
        "- Simple text\n",
        "- Academic text (with citations, URLs)\n",
        "- Social media text (with emojis, hashtags)\n",
        "- News text (formal writing)\n",
        "- Product reviews (informal, ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QNF6oBpFeV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b4d779-6918-4ea9-b155-0c2a2aaefb7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Sample texts loaded successfully!\n",
            "\n",
            "üè∑Ô∏è Simple: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "üè∑Ô∏è Academic: Dr. Smith's research on machine-learning algorithms is groundbreaking!\n",
            "She publi...\n",
            "\n",
            "üè∑Ô∏è Social Media: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yu...\n",
            "\n",
            "üè∑Ô∏è News: The stock market experienced significant volatility today, with tech stocks lead...\n",
            "\n",
            "üè∑Ô∏è Product Review: This laptop is absolutely fantastic! I've been using it for 6 months and it's st...\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Load Sample Texts\n",
        "simple_text = \"Natural Language Processing is a fascinating field of AI. It's amazing!\"\n",
        "\n",
        "academic_text = \"\"\"\n",
        "Dr. Smith's research on machine-learning algorithms is groundbreaking!\n",
        "She published 3 papers in 2023, focusing on deep neural networks (DNNs).\n",
        "The results were amazing - accuracy improved by 15.7%!\n",
        "\"This is revolutionary,\" said Prof. Johnson.\n",
        "Visit https://example.com for more info. #NLP #AI @university\n",
        "\"\"\"\n",
        "\n",
        "social_text = \"OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\"\n",
        "\n",
        "news_text = \"\"\"\n",
        "The stock market experienced significant volatility today, with tech stocks leading the decline.\n",
        "Apple Inc. (AAPL) dropped 3.2%, while Microsoft Corp. fell 2.8%.\n",
        "\"We're seeing a rotation out of growth stocks,\" said analyst Jane Doe from XYZ Capital.\n",
        "\"\"\"\n",
        "\n",
        "review_text = \"\"\"\n",
        "This laptop is absolutely fantastic! I've been using it for 6 months and it's still super fast.\n",
        "The battery life is incredible - lasts 8-10 hours easily.\n",
        "Only complaint: the keyboard could be better. Overall rating: 4.5/5 stars.\n",
        "\"\"\"\n",
        "\n",
        "# Store all texts\n",
        "sample_texts = {\n",
        "    \"Simple\": simple_text,\n",
        "    \"Academic\": academic_text.strip(),\n",
        "    \"Social Media\": social_text,\n",
        "    \"News\": news_text.strip(),\n",
        "    \"Product Review\": review_text.strip()\n",
        "}\n",
        "\n",
        "print(\"üìÑ Sample texts loaded successfully!\")\n",
        "for name, text in sample_texts.items():\n",
        "    preview = text[:80] + \"...\" if len(text) > 80 else text\n",
        "    print(f\"\\nüè∑Ô∏è {name}: {preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBOSIGXCeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 3\n",
        "**Looking at the different text types we've loaded, what preprocessing challenges do you anticipate for each type? For each text type below, identify at least 2 specific preprocessing challenges and explain why they might be problematic for NLP analysis.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Simple text challenges:**\n",
        "1.Lack of complexity ‚Äì The text is too short and straightforward, which may not provide enough context for deep analysis or training models.\n",
        "\n",
        "\n",
        "2.Contractions ‚Äì Words like \"It's\" need to be expanded to \"It is\" to avoid confusion during tokenization or parsing.\n",
        "\n",
        "\n",
        "\n",
        "**Academic text challenges:**\n",
        "1.Technical jargon and abbreviations ‚Äì Terms like \"DNNs\" or \"machine-learning algorithms\" might not be recognized without domain-specific models.\n",
        "2.Complex sentence structures ‚Äì Long, information-dense sentences with multiple clauses can be harder to parse accurately.\n",
        "\n",
        "**Social media text challenges:**\n",
        "1.Emojis and hashtags ‚Äì Symbols like \"‚òïÔ∏è\", \"üòç\", or \"#yum\" carry meaning but are difficult for standard models to interpret.\n",
        "2.Slang and informal language ‚Äì Words like \"OMG\" or phrases like \"SO GOOD!!!\" are non-standard and often require special preprocessing.\n",
        "\n",
        "**News text challenges:**\n",
        "1.Named entity recognition (NER) ‚Äì Identifying companies, stocks, and people correctly (e.g., \"Apple Inc.\", \"Jane Doe\") is crucial but can be tricky.\n",
        "2.Financial data and quotes ‚Äì Parsing numerical data (e.g., \"3.2%\") and attributing quotes to the right speakers requires careful structure analysis.\n",
        "\n",
        "\n",
        "\n",
        "**Product review challenges:**\n",
        "1.Mixed sentiment ‚Äì A review may contain both praise and criticism in the same text, making sentiment analysis more difficult.\n",
        "2.Informal phrasing ‚Äì Phrases like ‚Äúsuper fast‚Äù or ‚Äúcould be better‚Äù are subjective and can be hard to quantify accurately in analysis.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLLnDF9YeV9-"
      },
      "source": [
        "## üî§ Part 3: Tokenization\n",
        "\n",
        "### What is Tokenization?\n",
        "Tokenization is the process of breaking down text into smaller, meaningful units called **tokens**. These tokens are typically words, but can also be sentences, characters, or subwords.\n",
        "\n",
        "### Why is it Important?\n",
        "- Most NLP algorithms work with individual tokens, not entire texts\n",
        "- It's the foundation for all subsequent preprocessing steps\n",
        "- Different tokenization strategies can significantly impact results\n",
        "\n",
        "### Common Challenges:\n",
        "- **Contractions:** \"don't\" ‚Üí \"do\" + \"n't\" or \"don't\"?\n",
        "- **Punctuation:** Keep with words or separate?\n",
        "- **Special characters:** How to handle @, #, URLs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3T8WpUheV9-"
      },
      "outputs": [],
      "source": [
        "# Step 4: Tokenization with NLTK\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Test on simple text\n",
        "print(\"üîç NLTK Tokenization Results\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "\n",
        "# Word tokenization\n",
        "nltk_tokens = word_tokenize(simple_text)\n",
        "print(f\"\\nWord tokens: {nltk_tokens}\")\n",
        "print(f\"Number of tokens: {len(nltk_tokens)}\")\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(simple_text)\n",
        "print(f\"\\nSentences: {sentences}\")\n",
        "print(f\"Number of sentences: {len(sentences)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wlre1ymeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 4\n",
        "**Examine the NLTK tokenization results above. How did NLTK handle the contraction \"It's\"? What happened to the punctuation marks? Do you think this approach is appropriate for all NLP tasks? Explain your reasoning.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**How \"It's\" was handled:** NLTK split the contraction \"It's\" into two tokens: \"It\" and \"'s\". This is common in many NLP tokenizers because it helps identify the base word and the contraction separately, which can be useful for grammatical analysis.\n",
        "\n",
        "\n",
        "\n",
        "**Punctuation treatment:**Punctuation marks like the period and exclamation mark were treated as separate tokens. This allows the tokenizer to preserve sentence boundaries and makes it easier to analyze sentence structure or identify sentence-ending punctuation.\n",
        "\n",
        "\n",
        "\n",
        "**Appropriateness for different tasks:** This approach is appropriate for many NLP tasks like part-of-speech tagging, dependency parsing, and grammar analysis, where separating contractions and punctuation is helpful. However, for tasks like sentiment analysis or text generation, breaking contractions and punctuation might sometimes lose the natural flow or tone of the text.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQKNF-yceV9-"
      },
      "outputs": [],
      "source": [
        "# Step 5: Tokenization with spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"üîç spaCy Tokenization Results\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "\n",
        "# Process with spaCy\n",
        "doc = nlp(simple_text)\n",
        "\n",
        "# Extract tokens\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(f\"\\nWord tokens: {spacy_tokens}\")\n",
        "print(f\"Number of tokens: {len(spacy_tokens)}\")\n",
        "\n",
        "# Show detailed token information\n",
        "print(f\"\\nüî¨ Detailed Token Analysis:\")\n",
        "print(f\"{'Token':<12} {'POS':<8} {'Lemma':<12} {'Is Alpha':<8} {'Is Stop':<8}\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} {token.pos_:<8} {token.lemma_:<12} {token.is_alpha:<8} {token.is_stop:<8}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrQbVHceV9_"
      },
      "source": [
        "### ü§î Conceptual Question 5\n",
        "**Compare the NLTK and spaCy tokenization results. What differences do you notice? Which approach do you think would be better for different NLP tasks? Consider specific examples like sentiment analysis vs. information extraction.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Key differences observed:** Both NLTK and spaCy split contractions like ‚ÄúIt‚Äôs‚Äù into two tokens (It and 's). However, spaCy provides much richer information for each token, such as part of speech, lemmas, and stop word flags. NLTK focuses mainly on splitting the text into words and sentences without additional linguistic details. Also, spaCy‚Äôs tokenization tends to be more consistent with linguistic rules, especially for complex text.\n",
        "\n",
        "\n",
        "\n",
        "**Better for sentiment analysis:** NLTK‚Äôs simpler tokenization might be sufficient for basic sentiment analysis where the focus is mainly on words and their counts. However, spaCy‚Äôs ability to identify lemmas and stop words can improve sentiment models by reducing noise and capturing the true meaning of words. So, spaCy is generally better if a more nuanced understanding is needed.\n",
        "\n",
        "\n",
        "\n",
        "**Better for information extraction:** SpaCy is clearly better for information extraction tasks because it provides part ofspeech tags, lemmas, dependency parsing, and named entity recognition. These features allow it to identify relationships, entities, and roles in the text more accurately than NLTK‚Äôs basic tokenization.\n",
        "\n",
        "**Overall assessment:** NLTK is great for straightforward, lightweight tokenization tasks and quick experiments. SpaCy, on the other hand, offers a powerful and linguistically informed pipeline that is better suited for advanced NLP applications like entity recognition, syntactic parsing, and context-aware tasks.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCVNmEgseV9_"
      },
      "outputs": [],
      "source": [
        "# Step 6: Test Tokenization on Complex Text\n",
        "print(\"üß™ Testing on Social Media Text\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {social_text}\")\n",
        "\n",
        "# NLTK approach\n",
        "social_nltk_tokens = word_tokenize(social_text)\n",
        "print(f\"\\nNLTK tokens: {social_nltk_tokens}\")\n",
        "\n",
        "# spaCy approach\n",
        "social_doc = nlp(social_text)\n",
        "social_spacy_tokens = [token.text for token in social_doc]\n",
        "print(f\"spaCy tokens: {social_spacy_tokens}\")\n",
        "\n",
        "print(f\"\\nüìä Comparison:\")\n",
        "print(f\"NLTK token count: {len(social_nltk_tokens)}\")\n",
        "print(f\"spaCy token count: {len(social_spacy_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOYfP6KqeV9_"
      },
      "source": [
        "### ü§î Conceptual Question 6\n",
        "**Looking at how the libraries handled social media text (emojis, hashtags), which library seems more robust for handling \"messy\" real-world text? What specific advantages do you notice? How might this impact a real-world application like social media sentiment analysis?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**More robust library:** spaCy appears more robust for handling \"messy\" real-world text like social media posts.\n",
        "\n",
        "**Specific advantages:**\n",
        "\n",
        "*   spaCy treats emojis, hashtags, and mentions as separate, meaningful tokens, which helps capture the full context of social media language.\n",
        "\n",
        "* It provides detailed linguistic annotations, allowing better understanding of slang and informal expressions.\n",
        "\n",
        "* spaCy‚Äôs tokenizer is designed to handle non-standard text elements common in social media, such as repeated punctuation and emoticons.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Impact on sentiment analysis:** Because spaCy recognizes emojis and hashtags as distinct tokens, it can better capture the emotional tone and topics discussed in posts. This leads to more accurate sentiment detection and helps identify trends or opinions expressed in informal language, improving real-world social media monitoring and analysis.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkfnnYyweV9_"
      },
      "source": [
        "## üõë Part 4: Stop Words Removal\n",
        "\n",
        "### What are Stop Words?\n",
        "Stop words are common words that appear frequently in a language but typically don't carry much meaningful information about the content. Examples include \"the\", \"is\", \"at\", \"which\", \"on\", etc.\n",
        "\n",
        "### Why Remove Stop Words?\n",
        "1. **Reduce noise** in the data\n",
        "2. **Improve efficiency** by reducing vocabulary size\n",
        "3. **Focus on content words** that carry semantic meaning\n",
        "\n",
        "### When NOT to Remove Stop Words?\n",
        "- **Sentiment analysis:** \"not good\" vs \"good\" - the \"not\" is crucial!\n",
        "- **Question answering:** \"What is the capital?\" - \"what\" and \"is\" provide context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWwuapZ0eV9_"
      },
      "outputs": [],
      "source": [
        "# Step 7: Explore Stop Words Lists\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get NLTK English stop words\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "print(f\"üìä NLTK has {len(nltk_stopwords)} English stop words\")\n",
        "print(f\"First 20: {sorted(list(nltk_stopwords))[:20]}\")\n",
        "\n",
        "# Get spaCy stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words\n",
        "print(f\"\\nüìä spaCy has {len(spacy_stopwords)} English stop words\")\n",
        "print(f\"First 20: {sorted(list(spacy_stopwords))[:20]}\")\n",
        "\n",
        "# Compare the lists\n",
        "common_stopwords = nltk_stopwords.intersection(spacy_stopwords)\n",
        "nltk_only = nltk_stopwords - spacy_stopwords\n",
        "spacy_only = spacy_stopwords - nltk_stopwords\n",
        "\n",
        "print(f\"\\nüîç Comparison:\")\n",
        "print(f\"Common stop words: {len(common_stopwords)}\")\n",
        "print(f\"Only in NLTK: {len(nltk_only)} - Examples: {sorted(list(nltk_only))[:5]}\")\n",
        "print(f\"Only in spaCy: {len(spacy_only)} - Examples: {sorted(list(spacy_only))[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4rQc88leV9_"
      },
      "source": [
        "### ü§î Conceptual Question 7\n",
        "**Why do you think NLTK and spaCy have different stop word lists? Look at the examples of words that are only in one list - do you agree with these choices? Can you think of scenarios where these differences might significantly impact your NLP results?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Reasons for differences:** NLTK and spaCy have different stop word lists because they were created with different design goals and use cases in mind. NLTK‚Äôs list is more traditional and based on classic linguistic resources, while spaCy‚Äôs list is designed for modern NLP tasks and includes more contractions, abbreviations, and common informal words. Each toolkit tailors its stop words to what their users typically need.\n",
        "\n",
        "\n",
        "\n",
        "**Agreement with choices:** Some unique words in each list make sense given their purpose. For example, spaCy including contractions like ‚Äún't‚Äù helps remove common negations that may not carry strong meaning alone. On the other hand, NLTK‚Äôs inclusion of some archaic or less common words may not always be useful for everyday modern text. Overall, both lists have valid choices, but spaCy‚Äôs list might better suit contemporary language.\n",
        "\n",
        "\n",
        "**Scenarios where differences matter:** In sentiment analysis, removing negations like ‚Äúnot‚Äù or ‚Äún't‚Äù as stop words could change the sentiment drastically if not handled carefully.\n",
        "\n",
        "For topic modeling or information retrieval, missing or including certain stop words could affect keyword frequency and model quality.\n",
        "\n",
        "In social media analysis, where informal language and contractions are common, spaCy‚Äôs stop word list might perform better, whereas NLTK‚Äôs could be too broad or miss important tokens.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrGWsVMReV9_"
      },
      "outputs": [],
      "source": [
        "# Step 8: Remove Stop Words with NLTK\n",
        "# Test on simple text\n",
        "original_tokens = nltk_tokens  # From earlier tokenization\n",
        "filtered_tokens = [word for word in original_tokens if word.lower() not in nltk_stopwords]\n",
        "\n",
        "print(\"üß™ NLTK Stop Word Removal\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "print(f\"\\nOriginal tokens ({len(original_tokens)}): {original_tokens}\")\n",
        "print(f\"After removing stop words ({len(filtered_tokens)}): {filtered_tokens}\")\n",
        "\n",
        "# Show which words were removed\n",
        "removed_words = [word for word in original_tokens if word.lower() in nltk_stopwords]\n",
        "print(f\"\\nRemoved words: {removed_words}\")\n",
        "\n",
        "# Calculate reduction percentage\n",
        "reduction = (len(original_tokens) - len(filtered_tokens)) / len(original_tokens) * 100\n",
        "print(f\"Vocabulary reduction: {reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVuS0D-GeV9_"
      },
      "outputs": [],
      "source": [
        "# Step 9: Remove Stop Words with spaCy\n",
        "doc = nlp(simple_text)\n",
        "spacy_filtered = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(\"üß™ spaCy Stop Word Removal\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "print(f\"\\nOriginal tokens ({len(spacy_tokens)}): {spacy_tokens}\")\n",
        "print(f\"After removing stop words & punctuation ({len(spacy_filtered)}): {spacy_filtered}\")\n",
        "\n",
        "# Show which words were removed\n",
        "spacy_removed = [token.text for token in doc if token.is_stop or token.is_punct]\n",
        "print(f\"\\nRemoved words: {spacy_removed}\")\n",
        "\n",
        "# Calculate reduction percentage\n",
        "spacy_reduction = (len(spacy_tokens) - len(spacy_filtered)) / len(spacy_tokens) * 100\n",
        "print(f\"Vocabulary reduction: {spacy_reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoVxWinyeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 8\n",
        "**Compare the NLTK and spaCy stop word removal results. Which approach removed more words? Do you think removing punctuation (as spaCy did) is always a good idea? Give a specific example where keeping punctuation might be important for NLP analysis.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Which removed more:** spaCy removed more words overall because it filters out both stop words and punctuation, while the NLTK example removed only stop words and kept punctuation tokens.\n",
        "\n",
        "\n",
        "\n",
        "**Punctuation removal assessment:** Removing punctuation is often helpful to reduce noise, but it‚Äôs not always the best choice. Punctuation can carry important meaning, especially in informal texts or specialized domains.\n",
        "\n",
        "\n",
        "\n",
        "**Example where punctuation matters:** In sentiment analysis, exclamation marks can indicate strong emotions or emphasis, so keeping them helps the model understand intensity. For example, ‚ÄúI love this!‚Äù vs. ‚ÄúI love this.‚Äù convey different levels of enthusiasm.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUnesyUoeV-A"
      },
      "source": [
        "## üå± Part 5: Lemmatization and Stemming\n",
        "\n",
        "### What is Lemmatization?\n",
        "Lemmatization reduces words to their base or dictionary form (called a **lemma**). It considers context and part of speech to ensure the result is a valid word.\n",
        "\n",
        "### What is Stemming?\n",
        "Stemming reduces words to their root form by removing suffixes. It's faster but less accurate than lemmatization.\n",
        "\n",
        "### Key Differences:\n",
        "| Aspect | Stemming | Lemmatization |\n",
        "|--------|----------|---------------|\n",
        "| Speed | Fast | Slower |\n",
        "| Accuracy | Lower | Higher |\n",
        "| Output | May be non-words | Always valid words |\n",
        "| Context | Ignores context | Considers context |\n",
        "\n",
        "### Examples:\n",
        "- **\"running\"** ‚Üí Stem: \"run\", Lemma: \"run\"\n",
        "- **\"better\"** ‚Üí Stem: \"better\", Lemma: \"good\"\n",
        "- **\"was\"** ‚Üí Stem: \"wa\", Lemma: \"be\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhQlU7wGeV-A"
      },
      "outputs": [],
      "source": [
        "# Step 10: Stemming with NLTK\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Test words that demonstrate stemming challenges\n",
        "test_words = ['running', 'runs', 'ran', 'better', 'good', 'best', 'flying', 'flies', 'was', 'were', 'cats', 'dogs']\n",
        "\n",
        "print(\"üåø Stemming Demonstration\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"{'Original':<12} {'Stemmed':<12}\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "for word in test_words:\n",
        "    stemmed = stemmer.stem(word)\n",
        "    print(f\"{word:<12} {stemmed:<12}\")\n",
        "\n",
        "# Apply to our sample text\n",
        "sample_tokens = [token for token in nltk_tokens if token.isalpha()]\n",
        "stemmed_tokens = [stemmer.stem(token.lower()) for token in sample_tokens]\n",
        "\n",
        "print(f\"\\nüß™ Applied to sample text:\")\n",
        "print(f\"Original: {sample_tokens}\")\n",
        "print(f\"Stemmed: {stemmed_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1diItfT8eV-A"
      },
      "source": [
        "### ü§î Conceptual Question 9\n",
        "**Look at the stemming results above. Can you identify any cases where stemming produced questionable results? For example, how were \"better\" and \"good\" handled? Do you think this is problematic for NLP applications? Explain your reasoning.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Questionable results identified:** The stemmer reduced ‚Äúbetter‚Äù to ‚Äúbetter‚Äù, but ‚Äúgood‚Äù remained ‚Äúgood‚Äù as well, showing no common root. Also, words like ‚Äúrunning‚Äù and ‚Äúruns‚Äù were stemmed correctly to ‚Äúrun,‚Äù but ‚Äúflies‚Äù became ‚Äúfli,‚Äù which isn‚Äôt a proper root form.\n",
        "\n",
        "\n",
        "\n",
        "**Assessment of \"better\" and \"good\":** Since ‚Äúbetter‚Äù and ‚Äúgood‚Äù are related in meaning, the stemmer‚Äôs failure to link them is a limitation. The stemmer treats them as unrelated words because it relies on simple rule-based suffix stripping, not semantic understanding.\n",
        "\n",
        "\n",
        "\n",
        "**Impact on NLP applications:** This can be problematic for tasks like sentiment analysis or text classification, where recognizing related words matters. Stemming may oversimplify or miss important relationships, leading to less accurate results. In such cases, lemmatization or more advanced methods might be better.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYhyO8jfeV-A"
      },
      "outputs": [],
      "source": [
        "# Step 11: Lemmatization with spaCy\n",
        "print(\"üå± spaCy Lemmatization Demonstration\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Test on a complex sentence\n",
        "complex_sentence = \"The researchers were studying the effects of running and swimming on better performance.\"\n",
        "doc = nlp(complex_sentence)\n",
        "\n",
        "print(f\"Original: {complex_sentence}\")\n",
        "print(f\"\\n{'Token':<15} {'Lemma':<15} {'POS':<10} {'Explanation':<20}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_alpha:\n",
        "        explanation = \"No change\" if token.text.lower() == token.lemma_ else \"Lemmatized\"\n",
        "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10} {explanation:<20}\")\n",
        "\n",
        "# Extract lemmas\n",
        "lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "print(f\"\\nüî§ Lemmatized tokens (no stop words): {lemmas}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsCdhYHWeV-A"
      },
      "outputs": [],
      "source": [
        "# Step 12: Compare Stemming vs Lemmatization\n",
        "comparison_words = ['better', 'running', 'studies', 'was', 'children', 'feet']\n",
        "\n",
        "print(\"‚öñÔ∏è Stemming vs Lemmatization Comparison\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Original':<12} {'Stemmed':<12} {'Lemmatized':<12}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for word in comparison_words:\n",
        "    # Stemming\n",
        "    stemmed = stemmer.stem(word)\n",
        "\n",
        "    # Lemmatization with spaCy\n",
        "    doc = nlp(word)\n",
        "    lemmatized = doc[0].lemma_\n",
        "\n",
        "    print(f\"{word:<12} {stemmed:<12} {lemmatized:<12}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvNf3d6PeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 10\n",
        "**Compare the stemming and lemmatization results. Which approach do you think is more suitable for:**\n",
        "1. **A search engine** (where speed is crucial and you need to match variations of words)?\n",
        "2. **A sentiment analysis system** (where accuracy and meaning preservation are important)?\n",
        "3. **A real-time chatbot** (where both speed and accuracy matter)?\n",
        "\n",
        "**Explain your reasoning for each choice.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**1. Search engine:** Stemming is more suitable because it is faster and can quickly reduce words to their root forms, helping match different variations of a word during search queries. Speed is crucial here, and exact linguistic accuracy is less important than broad matching.\n",
        "\n",
        "**2. Sentiment analysis:** Lemmatization is better because it preserves the true meaning of words by considering context and grammar. This helps the system understand the sentiment more accurately, especially with irregular forms like ‚Äúbetter‚Äù vs. ‚Äúgood‚Äù or ‚Äúwas‚Äù vs. ‚Äúbe.‚Äù\n",
        "\n",
        "**3. Real-time chatbot:** A balance is needed. Lemmatization provides better accuracy for understanding user input, but stemming is faster. Using lemmatization with efficient implementation or selective stemming might work best to maintain reasonable speed without losing important meaning.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tep2Tdm4eV-A"
      },
      "source": [
        "## üßπ Part 6: Text Cleaning and Normalization\n",
        "\n",
        "### What is Text Cleaning?\n",
        "Text cleaning involves removing or standardizing elements that might interfere with analysis:\n",
        "- **Case normalization** (converting to lowercase)\n",
        "- **Punctuation removal**\n",
        "- **Number handling** (remove, replace, or normalize)\n",
        "- **Special character handling** (URLs, emails, mentions)\n",
        "- **Whitespace normalization**\n",
        "\n",
        "### Why is it Important?\n",
        "- Ensures consistency across your dataset\n",
        "- Reduces vocabulary size\n",
        "- Improves model performance\n",
        "- Handles edge cases in real-world data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmo2dAXkeV-A"
      },
      "outputs": [],
      "source": [
        "# Step 13: Basic Text Cleaning\n",
        "def basic_clean_text(text):\n",
        "    \"\"\"Apply basic text cleaning operations\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces again\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test basic cleaning\n",
        "test_text = \"   Hello WORLD!!! This has 123 numbers and   extra spaces.   \"\n",
        "cleaned = basic_clean_text(test_text)\n",
        "\n",
        "print(\"üßπ Basic Text Cleaning\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Original: '{test_text}'\")\n",
        "print(f\"Cleaned: '{cleaned}'\")\n",
        "print(f\"Length reduction: {(len(test_text) - len(cleaned))/len(test_text)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxHKo_2ceV-A"
      },
      "outputs": [],
      "source": [
        "# Step 14: Advanced Cleaning for Social Media\n",
        "def advanced_clean_text(text):\n",
        "    \"\"\"Apply advanced cleaning for social media and web text\"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Convert hashtags (keep the word, remove #)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    # Remove emojis (basic approach)\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Convert to lowercase and normalize whitespace\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test on social media text\n",
        "print(\"üöÄ Advanced Cleaning on Social Media Text\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"Original: {social_text}\")\n",
        "\n",
        "cleaned_social = advanced_clean_text(social_text)\n",
        "print(f\"Cleaned: {cleaned_social}\")\n",
        "print(f\"Length reduction: {(len(social_text) - len(cleaned_social))/len(social_text)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwtcrJMVeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 11\n",
        "**Look at the advanced cleaning results for the social media text. What information was lost during cleaning? Can you think of scenarios where removing emojis and hashtags might actually hurt your NLP application? What about scenarios where keeping them would be beneficial?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Information lost:** Emojis expressing emotions (like üòç, üëç) and hashtags that signal topics (#coffee, #yum) were removed, so the emotional tone and topical cues got lost.\n",
        "\n",
        "**Scenarios where removal hurts:** Sentiment analysis, where emojis often carry strong feelings.\n",
        "\n",
        "Trend detection or topic modeling, where hashtags identify key subjects or communities.\n",
        "\n",
        "\n",
        "\n",
        "**Scenarios where keeping helps:** When focusing on core text content only, like formal language processing or document classification.\n",
        "\n",
        "Reducing noise for models that don‚Äôt handle emojis/hashtags well, improving general text clarity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpzKWVy3eV-A"
      },
      "source": [
        "## üîß Part 7: Building a Complete Preprocessing Pipeline\n",
        "\n",
        "Now let's combine everything into a comprehensive preprocessing pipeline that you can customize based on your needs.\n",
        "\n",
        "### Pipeline Components:\n",
        "1. **Text cleaning** (basic or advanced)\n",
        "2. **Tokenization** (NLTK or spaCy)\n",
        "3. **Stop word removal** (optional)\n",
        "4. **Lemmatization/Stemming** (optional)\n",
        "5. **Additional filtering** (length, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XxqZCHneV-A"
      },
      "outputs": [],
      "source": [
        "# Step 15: Complete Preprocessing Pipeline\n",
        "def preprocess_text(text,\n",
        "                   clean_level='basic',     # 'basic' or 'advanced'\n",
        "                   remove_stopwords=True,\n",
        "                   use_lemmatization=True,\n",
        "                   use_stemming=False,\n",
        "                   min_length=2):\n",
        "    \"\"\"\n",
        "    Complete text preprocessing pipeline\n",
        "    \"\"\"\n",
        "    # Step 1: Clean text\n",
        "    if clean_level == 'basic':\n",
        "        cleaned_text = basic_clean_text(text)\n",
        "    else:\n",
        "        cleaned_text = advanced_clean_text(text)\n",
        "\n",
        "    # Step 2: Tokenize\n",
        "    if use_lemmatization:\n",
        "        # Use spaCy for lemmatization\n",
        "        doc = nlp(cleaned_text)\n",
        "        tokens = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
        "    else:\n",
        "        # Use NLTK for basic tokenization\n",
        "        tokens = word_tokenize(cleaned_text)\n",
        "        tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Step 3: Remove stop words\n",
        "    if remove_stopwords:\n",
        "        if use_lemmatization:\n",
        "            tokens = [token for token in tokens if token not in spacy_stopwords]\n",
        "        else:\n",
        "            tokens = [token.lower() for token in tokens if token.lower() not in nltk_stopwords]\n",
        "\n",
        "    # Step 4: Apply stemming if requested\n",
        "    if use_stemming and not use_lemmatization:\n",
        "        tokens = [stemmer.stem(token.lower()) for token in tokens]\n",
        "\n",
        "    # Step 5: Filter by length\n",
        "    tokens = [token for token in tokens if len(token) >= min_length]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "print(\"üîß Preprocessing Pipeline Created!\")\n",
        "print(\"‚úÖ Ready to test different configurations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYMlNDVceV-A"
      },
      "outputs": [],
      "source": [
        "# Step 16: Test Different Pipeline Configurations\n",
        "test_text = sample_texts[\"Product Review\"]\n",
        "print(f\"üéØ Testing on: {test_text[:100]}...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configuration 1: Minimal processing\n",
        "minimal = preprocess_text(test_text,\n",
        "                         clean_level='basic',\n",
        "                         remove_stopwords=False,\n",
        "                         use_lemmatization=False,\n",
        "                         use_stemming=False)\n",
        "print(f\"\\n1. Minimal processing ({len(minimal)} tokens):\")\n",
        "print(f\"   {minimal[:10]}...\")\n",
        "\n",
        "# Configuration 2: Standard processing\n",
        "standard = preprocess_text(test_text,\n",
        "                          clean_level='basic',\n",
        "                          remove_stopwords=True,\n",
        "                          use_lemmatization=True)\n",
        "print(f\"\\n2. Standard processing ({len(standard)} tokens):\")\n",
        "print(f\"   {standard[:10]}...\")\n",
        "\n",
        "# Configuration 3: Aggressive processing\n",
        "aggressive = preprocess_text(test_text,\n",
        "                            clean_level='advanced',\n",
        "                            remove_stopwords=True,\n",
        "                            use_lemmatization=False,\n",
        "                            use_stemming=True,\n",
        "                            min_length=3)\n",
        "print(f\"\\n3. Aggressive processing ({len(aggressive)} tokens):\")\n",
        "print(f\"   {aggressive[:10]}...\")\n",
        "\n",
        "# Show reduction percentages\n",
        "original_count = len(word_tokenize(test_text))\n",
        "print(f\"\\nüìä Token Reduction Summary:\")\n",
        "print(f\"   Original: {original_count} tokens\")\n",
        "print(f\"   Minimal: {len(minimal)} ({(original_count-len(minimal))/original_count*100:.1f}% reduction)\")\n",
        "print(f\"   Standard: {len(standard)} ({(original_count-len(standard))/original_count*100:.1f}% reduction)\")\n",
        "print(f\"   Aggressive: {len(aggressive)} ({(original_count-len(aggressive))/original_count*100:.1f}% reduction)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBb3LjQZeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 12\n",
        "**Compare the three pipeline configurations (Minimal, Standard, Aggressive). For each configuration, analyze:**\n",
        "1. **What information was preserved?**\n",
        "2. **What information was lost?**\n",
        "3. **What type of NLP task would this configuration be best suited for?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "Minimal Processing:\n",
        "\n",
        "Preserved:\n",
        "\n",
        "* All original words including stop words and common function words.\n",
        "* Full lexical variety and subtle nuances in word forms.\n",
        "* Punctuation removed but most text features intact.\n",
        "\n",
        "Lost:\n",
        "\n",
        "* Some noisy elements like punctuation and numbers.\n",
        "* No normalization or unification of word forms (e.g., running vs run).\n",
        "\n",
        "Best for:\n",
        "\n",
        "* Tasks requiring maximum context or detailed text understanding, like language modeling, text generation, or syntactic analysis.\n",
        "\n",
        "\n",
        "\n",
        "Standard Processing:\n",
        "\n",
        "Preserved:\n",
        "\n",
        "* Core content words with unified lemmas (e.g., ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù).\n",
        "* Reduced noise by removing stop words.\n",
        "* Maintains semantic meaning while simplifying word forms.\n",
        "\n",
        "Lost:\n",
        "\n",
        "* Stop words and some function words that might carry subtle meaning.\n",
        "* No emojis, URLs, or mentions removed (basic cleaning only).\n",
        "* Some nuance in original word forms lost due to lemmatization.\n",
        "\n",
        "Best for:\n",
        "\n",
        "* Sentiment analysis, topic modeling, text classification, or information retrieval where balance between noise reduction and meaning preservation is key.\n",
        "\n",
        "\n",
        "\n",
        "Aggressive Processing:\n",
        "\n",
        "Preserved:\n",
        "\n",
        "* Most meaningful content words after aggressive normalization and cleaning.\n",
        "* Removes social media noise like URLs, mentions, hashtags, and emojis.\n",
        "* Stemming reduces words to their root forms.\n",
        "\n",
        "Lost:\n",
        "\n",
        "* Stop words, short words, and any social media-specific signals like emojis and hashtags.\n",
        "* Detailed word form nuance due to stemming.\n",
        "* Potentially important context from social media-specific tokens removed.\n",
        "\n",
        "Best for:\n",
        "\n",
        "* Large-scale or real-time applications needing speed and compactness, such as spam filtering, large-scale search, or social media monitoring where noise is high and rapid processing is needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtgwLN7neV-A"
      },
      "outputs": [],
      "source": [
        "# Step 17: Comprehensive Analysis Across Text Types\n",
        "print(\"üî¨ Comprehensive Preprocessing Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test standard preprocessing on all text types\n",
        "results = {}\n",
        "for name, text in sample_texts.items():\n",
        "    original_tokens = len(word_tokenize(text))\n",
        "    processed_tokens = preprocess_text(text,\n",
        "                                      clean_level='basic',\n",
        "                                      remove_stopwords=True,\n",
        "                                      use_lemmatization=True)\n",
        "\n",
        "    reduction = (original_tokens - len(processed_tokens)) / original_tokens * 100\n",
        "    results[name] = {\n",
        "        'original': original_tokens,\n",
        "        'processed': len(processed_tokens),\n",
        "        'reduction': reduction,\n",
        "        'sample': processed_tokens[:8]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìÑ {name}:\")\n",
        "    print(f\"   Original: {original_tokens} tokens\")\n",
        "    print(f\"   Processed: {len(processed_tokens)} tokens ({reduction:.1f}% reduction)\")\n",
        "    print(f\"   Sample: {processed_tokens[:8]}\")\n",
        "\n",
        "# Summary table\n",
        "print(f\"\\n\\nüìã Summary Table\")\n",
        "print(f\"{'Text Type':<15} {'Original':<10} {'Processed':<10} {'Reduction':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for name, data in results.items():\n",
        "    print(f\"{name:<15} {data['original']:<10} {data['processed']:<10} {data['reduction']:<10.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY3hCn_7eV-A"
      },
      "source": [
        "### ü§î Final Conceptual Question 13\n",
        "**Looking at the comprehensive analysis results across all text types:**\n",
        "\n",
        "1. **Which text type was most affected by preprocessing?** Why do you think this happened?\n",
        "\n",
        "2. **Which text type was least affected?** What does this tell you about the nature of that text?\n",
        "\n",
        "3. **If you were building an NLP system to analyze customer reviews for a business, which preprocessing approach would you choose and why?**\n",
        "\n",
        "4. **What are the main trade-offs you need to consider when choosing preprocessing techniques for any NLP project?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**1. Most affected text type:**\n",
        "Social Media text was the most affected by preprocessing, showing the highest token reduction. This likely happened because social media posts often contain a lot of noise such as emojis, hashtags, mentions, URLs, and informal language, which the advanced cleaning and stop word removal target heavily.\n",
        "\n",
        "**2. Least affected text type:**\n",
        "Product Reviews or possibly Academic Papers were the least affected. These texts tend to be more formal, structured, and focused on conveying concrete information, so fewer tokens are removed during cleaning and stop word removal. This indicates that the language is more standardized and less noisy.\n",
        "\n",
        "**3. For customer review analysis:**\n",
        "A balanced preprocessing approach similar to the Standard Processing configuration would be ideal‚Äîcleaning text to remove noise and stop words while using lemmatization to preserve word meaning. This maintains important sentiment-bearing words and key terms without overly aggressive reduction, which is important for accurate sentiment and opinion analysis.\n",
        "\n",
        "**4. Main trade-offs to consider:**\n",
        "\n",
        "* **Information retention vs noise reduction:** Aggressive cleaning can remove useful context or subtle cues (e.g., emojis or punctuation), while minimal cleaning risks including irrelevant noise.\n",
        "* **Speed vs accuracy:** Simpler methods like stemming or minimal cleaning are faster but less accurate; lemmatization and advanced cleaning improve meaning but cost more processing time.\n",
        "* **Task-specific needs:** Some tasks require preserving certain tokens (e.g., hashtags in social media analysis), while others benefit from generalization (e.g., search engines).\n",
        "* **Domain and text type:** Informal text requires different cleaning than formal writing, affecting choice of preprocessing steps.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byb97qokeV-G"
      },
      "source": [
        "## üéØ Lab Summary and Reflection\n",
        "\n",
        "Congratulations! You've completed a comprehensive exploration of NLP preprocessing techniques.\n",
        "\n",
        "### üîë Key Concepts You've Mastered:\n",
        "\n",
        "1. **Text Preprocessing Fundamentals** - Understanding why preprocessing is crucial\n",
        "2. **Tokenization Techniques** - NLTK vs spaCy approaches and their trade-offs\n",
        "3. **Stop Word Management** - When to remove them and when to keep them\n",
        "4. **Morphological Processing** - Stemming vs lemmatization for different use cases\n",
        "5. **Text Cleaning Strategies** - Basic vs advanced cleaning for different text types\n",
        "6. **Pipeline Design** - Building modular, configurable preprocessing systems\n",
        "\n",
        "### üéì Real-World Applications:\n",
        "These techniques form the foundation for search engines, chatbots, sentiment analysis, document classification, machine translation, and information extraction systems.\n",
        "\n",
        "### üí° Key Insights to Remember:\n",
        "- **No Universal Solution**: Different NLP tasks require different preprocessing approaches\n",
        "- **Trade-offs Are Everywhere**: Balance information preservation with noise reduction\n",
        "- **Context Matters**: The same technique can help or hurt depending on your use case\n",
        "- **Experimentation Is Key**: Always test and measure impact on your specific task\n",
        "\n",
        "---\n",
        "\n",
        "**Excellent work completing Lab 02!** üéâ\n",
        "\n",
        "For your reflection journal, focus on the insights you gained about when and why to use different techniques, the challenges you encountered, and connections you made to real-world applications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}